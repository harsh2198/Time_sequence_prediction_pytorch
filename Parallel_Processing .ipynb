{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c49c0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "loss: 0.5085007548332214\n",
      "loss: 0.5055497288703918\n",
      "loss: 0.4758826494216919\n",
      "loss: 0.426965594291687\n",
      "loss: 0.030092421919107437\n",
      "loss: 0.16389000415802002\n",
      "loss: 0.04207533970475197\n",
      "loss: 0.11168678849935532\n",
      "loss: 0.025373734533786774\n",
      "loss: 0.024095721542835236\n",
      "loss: 0.02217981405556202\n",
      "loss: 0.019629525020718575\n",
      "loss: 0.01531327422708273\n",
      "loss: 0.009053901769220829\n",
      "loss: 0.007479482796043158\n",
      "loss: 0.006475199479609728\n",
      "loss: 0.004843422211706638\n",
      "loss: 0.0036760515067726374\n",
      "loss: 0.0030178632587194443\n",
      "loss: 0.0022949271369725466\n",
      "test loss: 0.0012232791632413864\n",
      "Step 1\n",
      "loss: 0.0012792543275281787\n",
      "loss: 0.0009511779644526541\n",
      "loss: 0.0008901028195396066\n",
      "loss: 0.0008836889755912125\n",
      "loss: 0.0008636777056381106\n",
      "loss: 0.0008194796973839402\n",
      "loss: 0.0006682518869638443\n",
      "loss: 0.0005867798463441432\n",
      "loss: 0.0005274274735711515\n",
      "loss: 0.0004594185738824308\n",
      "loss: 0.0004108198918402195\n",
      "loss: 0.0003806883469223976\n",
      "loss: 0.00036925444146618247\n",
      "loss: 0.0003662067756522447\n",
      "loss: 0.0003615817113313824\n",
      "loss: 0.0003532088012434542\n",
      "loss: 0.0003332017222419381\n",
      "loss: 0.0003176932514179498\n",
      "loss: 0.00030287387198768556\n",
      "loss: 0.000298387574730441\n",
      "test loss: 0.00027439362020231783\n",
      "Step 2\n",
      "loss: 0.0002968613407574594\n",
      "loss: 0.00029409362468868494\n",
      "loss: 0.00029020835063420236\n",
      "loss: 0.0002818206266965717\n",
      "loss: 0.00026576052187010646\n",
      "loss: 0.00024709603167138994\n",
      "loss: 0.00023370057169813663\n",
      "loss: 0.00022724387235939503\n",
      "loss: 0.0002224323689006269\n",
      "loss: 0.00022089618141762912\n",
      "loss: 0.00022031078697182238\n",
      "loss: 0.0002199917653342709\n",
      "loss: 0.00021945279149804264\n",
      "loss: 0.0002173026732634753\n",
      "loss: 0.00021239000489003956\n",
      "loss: 0.0002053854987025261\n",
      "loss: 0.00019468167738523334\n",
      "loss: 0.00018125007045455277\n",
      "loss: 0.00017540864064358175\n",
      "loss: 0.000165103207109496\n",
      "test loss: 0.00014780594210606068\n",
      "Step 3\n",
      "loss: 0.00016035250155255198\n",
      "loss: 0.00015521708701271564\n",
      "loss: 0.0001500135549576953\n",
      "loss: 0.0001412288547726348\n",
      "loss: 0.0001371401740470901\n",
      "loss: 0.00013548010610975325\n",
      "loss: 0.00013369705993682146\n",
      "loss: 0.00013150699669495225\n",
      "loss: 0.0001294734247494489\n",
      "loss: 0.00012667641567531973\n",
      "loss: 0.00012466100452002138\n",
      "loss: 0.0001231548230862245\n",
      "loss: 0.00012199141929158941\n",
      "loss: 0.00011953731154790148\n",
      "loss: 0.0001160809988505207\n",
      "loss: 0.00011425318371038884\n",
      "loss: 0.00011151993385283276\n",
      "loss: 0.00010773960821097717\n",
      "loss: 0.00010166321590077132\n",
      "loss: 9.914671682054177e-05\n",
      "test loss: 8.189397340174764e-05\n",
      "Step 4\n",
      "loss: 9.800643601920456e-05\n",
      "loss: 9.721126116346568e-05\n",
      "loss: 9.47427106439136e-05\n",
      "loss: 9.313487680628896e-05\n",
      "loss: 9.04284170246683e-05\n",
      "loss: 8.890622848412022e-05\n",
      "loss: 8.562191942473873e-05\n",
      "loss: 8.330640412168577e-05\n",
      "loss: 7.48157108318992e-05\n",
      "loss: 6.389559712260962e-05\n",
      "loss: 0.00018422909488435835\n",
      "loss: 0.00019224862626288086\n",
      "loss: 0.0006937418947927654\n",
      "loss: 0.0002404677652521059\n",
      "loss: 0.0001415902079315856\n",
      "loss: 0.0001104423645301722\n",
      "loss: 7.080599607434124e-05\n",
      "loss: 5.193916513235308e-05\n",
      "loss: 4.805208300240338e-05\n",
      "loss: 3.93725058529526e-05\n",
      "test loss: 3.171184653183445e-05\n",
      "Step 5\n",
      "loss: 3.589113839552738e-05\n",
      "loss: 3.265750274294987e-05\n",
      "loss: 3.1245606805896387e-05\n",
      "loss: 2.9779081160086207e-05\n",
      "loss: 2.884649256884586e-05\n",
      "loss: 2.704131111386232e-05\n",
      "loss: 2.5704282961669378e-05\n",
      "loss: 2.440765092615038e-05\n",
      "loss: 2.3639804567210376e-05\n",
      "loss: 2.315832352906e-05\n",
      "loss: 2.2983227609074675e-05\n",
      "loss: 2.2875294234836474e-05\n",
      "loss: 2.273592326673679e-05\n",
      "loss: 2.249294084322173e-05\n",
      "loss: 2.2022875782568008e-05\n",
      "loss: 2.1451511202030815e-05\n",
      "loss: 2.1013258447055705e-05\n",
      "loss: 2.0886827769572847e-05\n",
      "loss: 2.080870581266936e-05\n",
      "loss: 2.07677385333227e-05\n",
      "test loss: 2.0183604647172615e-05\n",
      "Step 6\n",
      "loss: 2.0740964828291908e-05\n",
      "loss: 2.0598301489371806e-05\n",
      "loss: 2.0467638023546897e-05\n",
      "loss: 2.025352841883432e-05\n",
      "loss: 2.009447052842006e-05\n",
      "loss: 2.0006860722787678e-05\n",
      "loss: 1.988663643714972e-05\n",
      "loss: 1.960431836778298e-05\n",
      "loss: 1.900913230201695e-05\n",
      "loss: 1.8304532204638235e-05\n",
      "loss: 1.7972735804505646e-05\n",
      "loss: 1.6997699276544154e-05\n",
      "loss: 1.611675907042809e-05\n",
      "loss: 1.528348911961075e-05\n",
      "loss: 1.4692443983221892e-05\n",
      "loss: 1.4388005183718633e-05\n",
      "loss: 1.3877861420041882e-05\n",
      "loss: 1.3713366570300423e-05\n",
      "loss: 1.3608858353109099e-05\n",
      "loss: 1.3549407412938308e-05\n",
      "test loss: 1.5954061382217333e-05\n",
      "Step 7\n",
      "loss: 1.3533091987483203e-05\n",
      "loss: 1.3492218386090826e-05\n",
      "loss: 1.3439504073176067e-05\n",
      "loss: 1.3380340533331037e-05\n",
      "loss: 1.3314106581674423e-05\n",
      "loss: 1.2963773770025e-05\n",
      "loss: 1.279626212635776e-05\n",
      "loss: 1.1893186638189945e-05\n",
      "loss: 1.3064705854048952e-05\n",
      "loss: 1.1108195394626819e-05\n",
      "loss: 1.0472876056155656e-05\n",
      "loss: 1.0100670806423295e-05\n",
      "loss: 1.3257646060083061e-05\n",
      "loss: 9.051093911693897e-06\n",
      "loss: 8.822131349006668e-06\n",
      "loss: 7.961441951920278e-06\n",
      "loss: 7.93208982940996e-06\n",
      "loss: 7.5909933912043925e-06\n",
      "loss: 7.516739515267545e-06\n",
      "loss: 7.301404366444331e-06\n",
      "test loss: 1.0532980923017021e-05\n",
      "Step 8\n",
      "loss: 7.140257821447449e-06\n",
      "loss: 7.112641469575465e-06\n",
      "loss: 6.834411124145845e-06\n",
      "loss: 6.7361302171775606e-06\n",
      "loss: 6.6098950810555834e-06\n",
      "loss: 6.604453574254876e-06\n",
      "loss: 6.4447795011801645e-06\n",
      "loss: 6.400910024240147e-06\n",
      "loss: 6.301371740846662e-06\n",
      "loss: 6.251054855965776e-06\n",
      "loss: 6.085704171709949e-06\n",
      "loss: 5.9530425460252445e-06\n",
      "loss: 5.712585334549658e-06\n",
      "loss: 5.771424184786156e-06\n",
      "loss: 5.59942054678686e-06\n",
      "loss: 5.583725396718364e-06\n",
      "loss: 5.567862444877392e-06\n",
      "loss: 5.557301847147755e-06\n",
      "loss: 5.5539690038131084e-06\n",
      "loss: 5.549323304876452e-06\n",
      "test loss: 8.971659553935751e-06\n",
      "Step 9\n",
      "loss: 5.540376605495112e-06\n",
      "loss: 5.512790721695637e-06\n",
      "loss: 5.4624870244879276e-06\n",
      "loss: 5.411056918092072e-06\n",
      "loss: 5.3853650570090394e-06\n",
      "loss: 5.374348347686464e-06\n",
      "loss: 5.3688509069615975e-06\n",
      "loss: 5.354738277674187e-06\n",
      "loss: 5.3269795898813754e-06\n",
      "loss: 5.289527052809717e-06\n",
      "loss: 5.245728971203789e-06\n",
      "loss: 5.18819251738023e-06\n",
      "loss: 5.09405890625203e-06\n",
      "loss: 5.01612521475181e-06\n",
      "loss: 4.900603471469367e-06\n",
      "loss: 4.841940153710311e-06\n",
      "loss: 4.807711320609087e-06\n",
      "loss: 4.7749781515449286e-06\n",
      "loss: 4.765792255057022e-06\n",
      "loss: 4.740295935334871e-06\n",
      "test loss: 5.857973519596271e-06\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 154\u001b[0m\n\u001b[1;32m    152\u001b[0m stop1\u001b[38;5;241m=\u001b[39mtimeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[1;32m    153\u001b[0m p\u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mcpu_count() \n\u001b[0;32m--> 154\u001b[0m sp \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m/\u001b[39m\u001b[43mtp\u001b[49m\n\u001b[1;32m    155\u001b[0m ef\u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of processors:\u001b[39m\u001b[38;5;124m\"\u001b[39m,p) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'tp' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import matplotlib.pyplot as plt \n",
    "import timeit \n",
    "import timeit \n",
    "import multiprocessing as mp \n",
    "\n",
    "#creating sin wave for input values \n",
    "T = 20\n",
    "L = 1000\n",
    "N = 100 \n",
    "\n",
    "x = np.empty((N, L), np.float32 )\n",
    "x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1) \n",
    "y = np.sin(x / 1.0 / T).astype(np.float32)\n",
    "\n",
    "#torch.save(data, open('traindata.pt', 'wb'))\n",
    "#creating and LSTM Predictor\n",
    "class Sequence(nn.Module):\n",
    "    def __init__(self,n_hidden = 51):\n",
    "        super(Sequence, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        #lstm cell1 & cells2, linear layer for prediction \n",
    "        self.lstm1 = nn.LSTMCell(1, self.n_hidden)\n",
    "        self.lstm2 = nn.LSTMCell(self.n_hidden, self.n_hidden ) \n",
    "        self.linear = nn.Linear(self.n_hidden, 1) \n",
    "        \n",
    "    def forward(self, x, future = 0): \n",
    "        outputs = []\n",
    "        n_samples = x.size(0)\n",
    "        h_t = torch.zeros(n_samples, self.n_hidden, dtype=torch.float32) \n",
    "        c_t = torch.zeros(n_samples, self.n_hidden, dtype=torch.float32) \n",
    "        h_t2 = torch.zeros(n_samples, self.n_hidden, dtype=torch.float32) \n",
    "        c_t2 = torch.zeros(n_samples, self.n_hidden, dtype=torch.float32) \n",
    "        for input_t in x.split(1, dim=1):\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t)) \n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2)) \n",
    "            output = self.linear(h_t2) \n",
    "            outputs.append(output) \n",
    "        # if we should predict the future \n",
    "        for i in range(future): \n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2) \n",
    "            outputs.append(output)\n",
    "        outputs = torch.cat(outputs, dim=1) \n",
    "        return outputs \n",
    "\n",
    "\n",
    "    #load data and make training set\n",
    "    # y= 100 , 1000\n",
    "train_input = torch.from_numpy(y[3:, :-1]) #97, 999 \n",
    "train_target = torch.from_numpy(y[3:, 1:]) #97, 999\n",
    "test_input = torch.from_numpy(y[:3, :-1]) # 3, 999\n",
    "test_target = torch.from_numpy(y[:3, 1:]) # 3, 999 \n",
    "    \n",
    "# build the model\n",
    "model = Sequence() \n",
    "criterion = nn.MSELoss() \n",
    "    \n",
    "# use LBFGS as optimizer since we can load the whole data to train \n",
    "optimizer = optim.LBFGS(model.parameters(), lr=0.8) \n",
    "\n",
    "start = timeit.default_timer() \n",
    "\n",
    "#begin to train \n",
    "n_steps = 10\n",
    "for i in range(n_steps):\n",
    "    print('Step', i) #change \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_input)\n",
    "        loss = criterion(out, train_target) \n",
    "        print('loss:', loss.item()) \n",
    "        loss.backward() #back propagation \n",
    "        return loss \n",
    "    optimizer.step(closure)\n",
    "            \n",
    "# begin to predict, no need to track gradient here \n",
    "    with torch.no_grad():\n",
    "        future = 1000 #predict steps can increase \n",
    "        pred = model(test_input, future=future) \n",
    "        loss = criterion(pred[:,:-future], test_target) \n",
    "        print('test loss:', loss.item())\n",
    "        y = pred.detach().numpy() # numerical outputs \n",
    "        \n",
    "    # draw the result \n",
    "    plt.figure(figsize=(12,6)) \n",
    "    plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)' f\"\\nstep{i+1}\", fontsize=20) \n",
    "    plt.xlabel('x', fontsize=20)\n",
    "    plt.ylabel('y', fontsize=20) \n",
    "    plt.xticks(fontsize=20) \n",
    "    plt.yticks(fontsize=20) \n",
    "    n = train_input.shape[1] #999 \n",
    "    def draw(yi, color):\n",
    "        plt.plot(np.arange(n), yi[:n], color, linewidth = 2.0) \n",
    "        plt.plot(np.arange(n, n+future), yi[n:], color + ':', linewidth = 2.0) \n",
    "    draw(y[0], 'r')\n",
    "    draw(y[1], 'g')\n",
    "    draw(y[2], 'b') \n",
    "    plt.savefig('predict%d.pdf'%i) \n",
    "    plt.close() \n",
    "\n",
    "stop = timeit.default_timer()\n",
    "t1=stop-start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc88e812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of processors: 2\n",
      "Time of serial processing: 286.108437525\n",
      "Time of parallel processing: 0.002728162000039447\n",
      "Speedup: 104872.23175194989\n",
      "Efficiency: 26218.057937987473\n"
     ]
    }
   ],
   "source": [
    "#Parallel processing \n",
    "\n",
    "pool= mp.Pool(mp.cpu_count())\n",
    "start1=timeit.default_timer()\n",
    "\n",
    "def parallel_pool(model,criterion,optimizer,train_input,train_target,test_input,test_target):\n",
    "    n_steps = 10\n",
    "    for i in range(n_steps):\n",
    "        print('Step', i) #change \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_input)\n",
    "        loss = criterion(out, train_target) \n",
    "        print('loss:', loss.item()) \n",
    "        loss.backward() #back propagation \n",
    "        return loss \n",
    "    optimizer.step(closure)\n",
    "            \n",
    "# begin to predict, no need to track gradient here \n",
    "    with torch.no_grad():\n",
    "        future = 1000 #predict steps can increase \n",
    "        pred = model(test_input, future=future) \n",
    "        loss = criterion(pred[:,:-future], test_target) \n",
    "        print('test loss:', loss.item())\n",
    "        y = pred.detach().numpy() # numerical outputs \n",
    "        \n",
    "    # draw the result \n",
    "    plt.figure(figsize=(12,6)) \n",
    "    plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)' f\"\\nstep{i+1}\", fontsize=20) \n",
    "    plt.xlabel('x', fontsize=20)\n",
    "    plt.ylabel('y', fontsize=20) \n",
    "    plt.xticks(fontsize=20) \n",
    "    plt.yticks(fontsize=20) \n",
    "    n = train_input.shape[1] #999 \n",
    "    def draw(yi, color):\n",
    "        plt.plot(np.arange(n), yi[:n], color, linewidth = 2.0) \n",
    "        plt.plot(np.arange(n, n+future), yi[n:], color + ':', linewidth = 2.0) \n",
    "    draw(y[0], 'r')\n",
    "    draw(y[1], 'g')\n",
    "    draw(y[2], 'b') \n",
    "    #plt.savefig('predict%d.pdf'%i) \n",
    "    plt.close() \n",
    "pool.close()\n",
    "stop1=timeit.default_timer()\n",
    "tp=stop1-start1\n",
    "p= mp.cpu_count() \n",
    "sp = t1/tp\n",
    "ef= sp/4\n",
    "print(\"number of processors:\",p) \n",
    "print(\"Time of serial processing:\",t1) \n",
    "print(\"Time of parallel processing:\",tp) \n",
    "print(\"Speedup:\",sp) \n",
    "print(\"Efficiency:\",ef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d792d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
