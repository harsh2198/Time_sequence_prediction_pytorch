{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c25f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "loss: 0.5117824673652649\n",
      "loss: 0.502273440361023\n",
      "loss: 0.48187577724456787\n",
      "loss: 0.4698578119277954\n",
      "loss: 0.4158863425254822\n",
      "loss: 23.42149543762207\n",
      "loss: 0.31355908513069153\n",
      "loss: 0.20930740237236023\n",
      "loss: 0.03968942165374756\n",
      "loss: 0.02204533852636814\n",
      "loss: 0.021877802908420563\n",
      "loss: 0.021036725491285324\n",
      "loss: 0.019406547769904137\n",
      "loss: 0.01629868522286415\n",
      "loss: 0.011432593688368797\n",
      "loss: 0.015418821945786476\n",
      "loss: 0.005477991886436939\n",
      "loss: 0.004696030635386705\n",
      "loss: 0.002819222630932927\n",
      "loss: 0.001863868092186749\n",
      "test loss: 0.0012473760871216655\n",
      "Step 1\n",
      "loss: 0.0012726872228085995\n",
      "loss: 0.0010519188363105059\n",
      "loss: 0.0008675462449900806\n",
      "loss: 0.0007738539716228843\n",
      "loss: 0.0006875462713651359\n",
      "loss: 0.0005840663216076791\n",
      "loss: 0.000494161038659513\n",
      "loss: 0.00047164628631435335\n",
      "loss: 0.00046624866081401706\n",
      "loss: 0.0004633176722563803\n",
      "loss: 0.00045721096103079617\n",
      "loss: 0.0004441509081516415\n",
      "loss: 0.000416094291722402\n",
      "loss: 0.0003601006173994392\n",
      "loss: 0.00026444756076671183\n",
      "loss: 0.0001883634104160592\n",
      "loss: 0.00017567278700880706\n",
      "loss: 0.0001643732248339802\n",
      "loss: 0.0001623074058443308\n",
      "loss: 0.00016185651475097984\n",
      "test loss: 0.00014920737885404378\n",
      "Step 2\n",
      "loss: 0.0001616480149095878\n",
      "loss: 0.00016142043750733137\n",
      "loss: 0.00016134059114847332\n",
      "loss: 0.0001612425985513255\n",
      "loss: 0.00016107918054331094\n",
      "loss: 0.0001607657177373767\n",
      "loss: 0.0001600827235961333\n",
      "loss: 0.00015855558740440756\n",
      "loss: 0.00015528952644672245\n",
      "loss: 0.00014978210674598813\n",
      "loss: 0.00014366392861120403\n",
      "loss: 0.0001349522062810138\n",
      "loss: 0.00013032132119406015\n",
      "loss: 0.00012859748676419258\n",
      "loss: 0.00012776823132298887\n",
      "loss: 0.000127445935504511\n",
      "loss: 0.00012649262498598546\n",
      "loss: 0.00012618012260645628\n",
      "loss: 0.00012572835839819163\n",
      "loss: 0.0001236977695953101\n",
      "test loss: 0.00010999548248946667\n",
      "Step 3\n",
      "loss: 0.00011991494102403522\n",
      "loss: 0.0001126858769566752\n",
      "loss: 0.00010037387255579233\n",
      "loss: 8.957306999946013e-05\n",
      "loss: 9.087659418582916e-05\n",
      "loss: 7.634446956217289e-05\n",
      "loss: 7.287890912266448e-05\n",
      "loss: 6.902944733155891e-05\n",
      "loss: 6.620310159632936e-05\n",
      "loss: 6.382691208273172e-05\n",
      "loss: 6.293723708949983e-05\n",
      "loss: 6.274254701565951e-05\n",
      "loss: 6.261181260924786e-05\n",
      "loss: 6.2385959608946e-05\n",
      "loss: 6.183287041494623e-05\n",
      "loss: 6.023240348440595e-05\n",
      "loss: 5.700780820916407e-05\n",
      "loss: 5.3392457630252466e-05\n",
      "loss: 5.0353508413536474e-05\n",
      "loss: 7.47056765248999e-05\n",
      "test loss: 4.794661799678579e-05\n",
      "Step 4\n",
      "loss: 4.9078000301960856e-05\n",
      "loss: 4.8654219426680356e-05\n",
      "loss: 4.836594598600641e-05\n",
      "loss: 4.707316111307591e-05\n",
      "loss: 4.6210636355681345e-05\n",
      "loss: 4.480997449718416e-05\n",
      "loss: 4.140449527767487e-05\n",
      "loss: 4.061909203301184e-05\n",
      "loss: 3.9083053707145154e-05\n",
      "loss: 3.854147871606983e-05\n",
      "loss: 3.539682074915618e-05\n",
      "loss: 3.421279325266369e-05\n",
      "loss: 3.325681609567255e-05\n",
      "loss: 3.2701482268748805e-05\n",
      "loss: 3.224659667466767e-05\n",
      "loss: 3.214987009414472e-05\n",
      "loss: 3.205484608770348e-05\n",
      "loss: 3.195837780367583e-05\n",
      "loss: 3.1656316423323005e-05\n",
      "loss: 3.102524351561442e-05\n",
      "test loss: 2.5200017262250185e-05\n",
      "Step 5\n",
      "loss: 2.968895023514051e-05\n",
      "loss: 2.7090676667285152e-05\n",
      "loss: 2.4450968339806423e-05\n",
      "loss: 2.293526631547138e-05\n",
      "loss: 2.244676034024451e-05\n",
      "loss: 2.1782445401186123e-05\n",
      "loss: 2.1569472664850764e-05\n",
      "loss: 2.144183781638276e-05\n",
      "loss: 2.1411313355201855e-05\n",
      "loss: 2.138995478162542e-05\n",
      "loss: 2.1314896002877504e-05\n",
      "loss: 2.096570642606821e-05\n",
      "loss: 2.0318078895797953e-05\n",
      "loss: 1.9057077224715613e-05\n",
      "loss: 1.7559515981702134e-05\n",
      "loss: 1.6000181858544238e-05\n",
      "loss: 1.614238317415584e-05\n",
      "loss: 1.2858216905442532e-05\n",
      "loss: 1.2202791367599275e-05\n",
      "loss: 1.1755733794416301e-05\n",
      "test loss: 9.038607458933257e-06\n",
      "Step 6\n",
      "loss: 1.137403251050273e-05\n",
      "loss: 1.1120557246613316e-05\n",
      "loss: 1.1032767361029983e-05\n",
      "loss: 1.0852406376216095e-05\n",
      "loss: 1.0742420272435993e-05\n",
      "loss: 1.0695320270315278e-05\n",
      "loss: 1.0631065379129723e-05\n",
      "loss: 1.0497970833966974e-05\n",
      "loss: 1.0237121387035586e-05\n",
      "loss: 9.886746738629881e-06\n",
      "loss: 9.549122296448331e-06\n",
      "loss: 9.144108844338916e-06\n",
      "loss: 8.955983503255993e-06\n",
      "loss: 8.828422323858831e-06\n",
      "loss: 8.580613211961463e-06\n",
      "loss: 8.365776011487469e-06\n",
      "loss: 7.85076008469332e-06\n",
      "loss: 7.5914304034085944e-06\n",
      "loss: 7.46559180697659e-06\n",
      "loss: 7.208288934634766e-06\n",
      "test loss: 6.031209977663821e-06\n",
      "Step 7\n",
      "loss: 7.119586371118203e-06\n",
      "loss: 7.0168148340599146e-06\n",
      "loss: 6.86097655488993e-06\n",
      "loss: 6.763221790606622e-06\n",
      "loss: 6.624881280004047e-06\n",
      "loss: 6.540846698044334e-06\n",
      "loss: 6.504140856122831e-06\n",
      "loss: 6.466257673309883e-06\n",
      "loss: 6.4559021666354965e-06\n",
      "loss: 6.451627996284515e-06\n",
      "loss: 6.44442343400442e-06\n",
      "loss: 6.427533207897795e-06\n",
      "loss: 6.389388090610737e-06\n",
      "loss: 6.308500360319158e-06\n",
      "loss: 6.213991127879126e-06\n",
      "loss: 6.152452442620415e-06\n",
      "loss: 6.07165884503047e-06\n",
      "loss: 6.012811809341656e-06\n",
      "loss: 5.980760761303827e-06\n",
      "test loss: 6.294662853179034e-06\n",
      "Step 8\n",
      "loss: 5.886827239010017e-06\n",
      "loss: 5.794975550088566e-06\n",
      "loss: 5.7529537116352e-06\n",
      "loss: 5.748647708969656e-06\n",
      "loss: 5.701277132175164e-06\n",
      "loss: 5.696287644241238e-06\n",
      "loss: 5.693988441635156e-06\n",
      "loss: 5.692914783139713e-06\n",
      "loss: 5.692011200153502e-06\n",
      "test loss: 5.628457984130364e-06\n",
      "Step 9\n",
      "loss: 5.692011200153502e-06\n",
      "loss: 5.690688340109773e-06\n",
      "loss: 5.688811143045314e-06\n",
      "loss: 5.68647510590381e-06\n",
      "loss: 5.684625193680404e-06\n",
      "loss: 5.6826115724106785e-06\n",
      "loss: 5.672035058523761e-06\n",
      "loss: 5.613006578641944e-06\n",
      "loss: 5.51623952560476e-06\n",
      "loss: 5.37017058377387e-06\n",
      "loss: 5.11718963025487e-06\n",
      "loss: 4.963557785231387e-06\n",
      "loss: 7.348719009314664e-06\n",
      "loss: 4.750446350954007e-06\n",
      "loss: 4.67551535621169e-06\n",
      "loss: 4.636317044059979e-06\n",
      "loss: 4.603184606821742e-06\n",
      "loss: 4.56022598882555e-06\n",
      "loss: 4.465492565941531e-06\n",
      "loss: 4.4034491111233365e-06\n",
      "test loss: 3.049390898013371e-06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import matplotlib.pyplot as plt \n",
    "import timeit \n",
    "\n",
    "#creating sin wave for input values \n",
    "T = 20\n",
    "L = 1000\n",
    "N = 100 \n",
    "\n",
    "x = np.empty((N, L), np.float32 )\n",
    "x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1) \n",
    "y = np.sin(x / 1.0 / T).astype(np.float32)\n",
    "\n",
    "#torch.save(data, open('traindata.pt', 'wb'))\n",
    "#creating and LSTM Predictor\n",
    "class Sequence(nn.Module):\n",
    "    def __init__(self,n_hidden = 51):\n",
    "        super(Sequence, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        #lstm cell1 & cells2, linear layer for prediction \n",
    "        self.lstm1 = nn.LSTMCell(1, self.n_hidden)\n",
    "        self.lstm2 = nn.LSTMCell(self.n_hidden, self.n_hidden ) \n",
    "        self.linear = nn.Linear(self.n_hidden, 1) \n",
    "        \n",
    "    def forward(self, x, future = 0): \n",
    "        outputs = []\n",
    "        n_samples = x.size(0)\n",
    "        h_t = torch.zeros(n_samples, self.n_hidden, dtype=torch.float32) \n",
    "        c_t = torch.zeros(n_samples, self.n_hidden, dtype=torch.float32) \n",
    "        h_t2 = torch.zeros(n_samples, self.n_hidden, dtype=torch.float32) \n",
    "        c_t2 = torch.zeros(n_samples, self.n_hidden, dtype=torch.float32) \n",
    "        for input_t in x.split(1, dim=1):\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t)) \n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2)) \n",
    "            output = self.linear(h_t2) \n",
    "            outputs.append(output) \n",
    "        # if we should predict the future \n",
    "        for i in range(future): \n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2) \n",
    "            outputs.append(output)\n",
    "        outputs = torch.cat(outputs, dim=1) \n",
    "        return outputs \n",
    "\n",
    "\n",
    "    #load data and make training set\n",
    "    # y= 100 , 1000\n",
    "train_input = torch.from_numpy(y[3:, :-1]) #97, 999 \n",
    "train_target = torch.from_numpy(y[3:, 1:]) #97, 999\n",
    "test_input = torch.from_numpy(y[:3, :-1]) # 3, 999\n",
    "test_target = torch.from_numpy(y[:3, 1:]) # 3, 999 \n",
    "    \n",
    "# build the model\n",
    "model = Sequence() \n",
    "criterion = nn.MSELoss() \n",
    "    \n",
    "# use LBFGS as optimizer since we can load the whole data to train \n",
    "optimizer = optim.LBFGS(model.parameters(), lr=0.8) \n",
    "start = timeit.default_timer() \n",
    "    \n",
    "#begin to train \n",
    "n_steps = 10\n",
    "for i in range(n_steps):\n",
    "    print('Step', i) #change \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_input)\n",
    "        loss = criterion(out, train_target) \n",
    "        print('loss:', loss.item()) \n",
    "        loss.backward() #back propagation \n",
    "        return loss \n",
    "    optimizer.step(closure)\n",
    "            \n",
    "# begin to predict, no need to track gradient here \n",
    "    with torch.no_grad():\n",
    "        future = 1000 #predict steps can increase \n",
    "        pred = model(test_input, future=future) \n",
    "        loss = criterion(pred[:,:-future], test_target) \n",
    "        print('test loss:', loss.item())\n",
    "        y = pred.detach().numpy() # numerical outputs \n",
    "        \n",
    "    # draw the result \n",
    "    plt.figure(figsize=(12,8)) \n",
    "    plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)' f\"\\nstep{i+1}\", fontsize=20) \n",
    "    plt.xlabel('x', fontsize=15)\n",
    "    plt.ylabel('y', fontsize=15) \n",
    "    plt.xticks(fontsize=15) \n",
    "    plt.yticks(fontsize=15) \n",
    "    n = train_input.shape[1] #999 \n",
    "    def draw(yi, color):\n",
    "        plt.plot(np.arange(n), yi[:n], color, linewidth = 2.0) \n",
    "        plt.plot(np.arange(n, n+future), yi[n:], color + ':', linewidth = 2.0) \n",
    "    draw(y[0], 'r')\n",
    "    draw(y[1], 'g')\n",
    "    draw(y[2], 'b') \n",
    "    plt.savefig('predict%d.pdf'%i) \n",
    "    plt.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75651dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3f027b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
